BUDDY AI Assistant - Current Implementation Overview
Generated on July 01, 2025

PROJECT STATUS (as of July 2025):
- Project B.U.D.D.Y. is a cross-platform AI assistant for Mac and iPhone.
- The backend uses open-source LLMs (Ollama) running locally for chat.
- Image generation is handled by a single Hugging Face Space (CPU-based).
- Images are uploaded to Cloudinary for storage and delivery.
- The frontend is a SwiftUI app that communicates with the local backend.
- The backend acts as a proxy, forwarding requests to appropriate services.

CURRENT ARCHITECTURE:

1. BACKEND: FastAPI (Python) running locally
   - Acts as a lightweight proxy server
   - Forwards chat requests to local Ollama
   - Forwards image requests to Hugging Face Space
   - Uploads generated images to Cloudinary
   - Runs with 4 workers for parallel requests

2. LLM CHAT: Ollama running locally
   - Uses models like Mistral, TinyLlama, Phi-2
   - Runs on localhost:11434
   - Provides instant, private chat responses
   - No cloud costs or dependencies

3. IMAGE GENERATION: Hugging Face Space
   - Single CPU-based space for image generation
   - URL: https://chiefmaybe-buddy-sd.hf.space/generate
   - Generation time: 5-10 minutes
   - Free tier hosting

4. IMAGE STORAGE: Cloudinary
   - Stores generated images
   - Provides CDN delivery
   - Free tier usage

5. FRONTEND: SwiftUI App
   - iOS and macOS compatible
   - Communicates with local backend
   - Displays chat responses and generated images

CURRENT WORKFLOW:

Chat Flow:
- User enters prompt in SwiftUI app
- Backend forwards to local Ollama
- Ollama generates response
- Response displayed in app

Image Generation Flow:
- User enters image prompt in SwiftUI app
- Backend forwards to Hugging Face Space
- Space generates image (5-10 minutes)
- Backend uploads image to Cloudinary
- Cloudinary URL returned to app
- Image displayed in app

SETUP REQUIREMENTS:

Local Development:
- MacBook Pro M1 (16GB RAM recommended)
- Python environment with FastAPI
- Ollama installed and running
- Xcode for SwiftUI development

Cloud Services:
- Hugging Face account (free)
- Cloudinary account (free tier)
- Environment variables for Cloudinary credentials

CURRENT STATUS:
- ✅ Local chat pipeline working
- ✅ Cloud image generation working
- ✅ SwiftUI frontend connected
- ✅ Cloudinary integration working
- ✅ Backend proxy functionality working

TECHNICAL SPECIFICATIONS:

Backend (FastAPI):
- Port: 8000
- Workers: 4 (parallel requests)
- Dependencies: FastAPI, requests, cloudinary
- No local ML libraries required

Ollama:
- Port: 11434
- Models: Mistral, TinyLlama, Phi-2 (open source)
- Local processing only

Hugging Face Space:
- Runtime: Docker
- Hardware: CPU
- Model: Stable Diffusion v1.4
- Timeout: 33 minutes

Cloudinary:
- Usage: Image storage only
- No transformations or manipulations
- Free tier limits

FRONTEND SPECIFICATIONS:

SwiftUI App:
- Target: iOS and macOS
- Network: localhost:8000 for backend
- Features: Chat interface, image generation, modal display
- Permissions: Network access for localhost

ENVIRONMENT VARIABLES:
- CLOUDINARY_CLOUD_NAME
- CLOUDINARY_API_KEY
- CLOUDINARY_API_SECRET

DEPLOYMENT NOTES:
- Backend runs locally for development
- Ollama runs locally for chat
- Image generation uses free cloud resources
- No paid cloud services required for current setup

FUTURE CONSIDERATIONS:
- Containerize backend and Ollama with Docker
- Deploy to paid cloud server when ready
- Move environment variables to cloud host
- Update frontend to use cloud API endpoint

This implementation provides a complete, working AI assistant using free and open-source tools with local-first chat and cloud-based image generation. 