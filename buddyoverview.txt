Prompt to Build a Free AI Assistant App (B.U.D.D.Y.) Using Cursor AI,
Ollama, and Cloud Tools
Generated on July 01, 2025
This prompt is designed to help you use Cursor AI to build your own Buddy-like AI assistant entirely
for free. It integrates local or cloud-based LLMs, image generation, voice-to-text, and a mobile
frontend, all using free-tier or open-source tools.

PROJECT PROGRESS SUMMARY (as of July 2025):
- Project B.U.D.D.Y. is being developed as a cross-platform AI assistant for Mac and iPhone.
- The backend is designed to use open-source LLMs (Ollama) and Stable Diffusion for image generation.
- Voice-to-text is planned via Whisper.cpp or iOS Speech API.
- The image delivery pipeline is set up to upload generated images to Cloudinary, with the frontend displaying images from Cloudinary URLs.
- The frontend is being built to forward prompts to the backend and display both text and image responses in a modal container.
- The backend API is being developed with FastAPI for easy integration and deployment.
- Requirements and compatibility are being carefully managed to avoid runtime errors, especially with diffusers and huggingface_hub versions.
- The project is structured for cloud deployment, with a focus on free-tier and open-source tools.

PROMPT FOR CURSOR AI:
You are an expert developer helping me create a personal AI assistant app (like
Buddy from Iron Man), using only free or open-source technologies. The app
should be accessible from my iPhone and Mac, run in the cloud (no local
always-on server required), and respond to voice or typed prompts. It must also
generate images on request.
Here's what I need you to build, step-by-step:
1. BACKEND: Deploy a small open-source LLM (like OpenHermes, Dolphin, or
Mistral) using Ollama and host it on a free-tier cloud platform (Fly.io, Hugging
Face Spaces, or Render).
- Use CPU-based models to avoid GPU costs
- Include endpoints for chat prompt input and output
2. SPEECH-TO-TEXT: Add Whisper.cpp or iOS voice recognition API for free
voice-to-text.
- Integrate this into the mobile app for live prompting
3. IMAGE GENERATION: Integrate Stable Diffusion hosted on Hugging Face Spaces
(free) or locally using InvokeAI for testing.
- When the user prompts for an image, the backend should call the Stable
Diffusion API and return a generated image
4. IMAGE DELIVERY: Upload images to Cloudinary using their free tier.
- Automatically create modals in the app frontend to display the image
- No URL clicking required-images should display inline
5. FRONTEND: Use React Native or Expo to build a mobile app interface.
- Must work on iPhone
- Voice or text input should trigger backend API
- Display assistant's text and image responses directly in the UI
6. CLOUD-BASED B.U.D.D.Y.: Ensure the entire system runs on a cloud backend so I can
prompt Buddy from anywhere, anytime, without needing my Mac to be on.
7. OPTIONAL: Add authentication via Clerk.dev or Supabase (free tiers) and store
basic prompt history or image log.
Additional Notes:
- Include configuration for automatic deployment and basic error handling
- Keep everything in a single GitHub repo and provide a one-command launch
(Docker or shell script)

Building a Local AI Assistant App with Ollama + Stable Diffusion
Overview:
You can build a cross-platform (Mac + iPhone) AI assistant app that runs Ollama as the backend
LLM, with optional Stable Diffusion image generation—all without needing to use the Terminal for
day-to-day tasks.
Tools You'll Use:
- Cursor AI: Writes code for you, scaffolds Swift/Xcode projects.
- Ollama: Local LLM backend (e.g., Dolphin, Mistral).
- Stable Diffusion: Local image generation (e.g., via DiffusionBee).
- Whisper or Apple Speech API: For voice input.
- Xcode: To compile and run your Mac/iOS app.
Basic Architecture:
1. Mac runs:
- Ollama (on localhost:11434)
- Stable Diffusion (via DiffusionBee or web UI like InvokeAI)
2. iPhone app (Swift) sends text or voice prompts to your Mac via local network.
3. The app receives responses from Ollama or images from SD and displays them.
4. Optional: Use a tiny local API (Node.js or Python) as an intermediary.
Features Your App Can Have:
- A clean chat UI (like ChatGPT) that communicates with Ollama.
- Button to toggle between text or voice input (via Whisper or native iOS API).
- Image generator tab: Sends prompts to Stable Diffusion and shows results.
- Settings screen: Choose LLM model, switch to creative/image mode, etc.
Feasibility:
- YES: Cursor AI can generate most of this scaffolding automatically.
- YES: Ollama works perfectly on M1/M2 Macs.
- YES: DiffusionBee works locally and integrates easily.
- Caveat: iPhones can't run Ollama/SD locally, so must use Wi-Fi to connect to Mac.
Next Steps:
1. Use Cursor AI to scaffold a Swift app that uses URLSession to send requests to localhost.
2. Set up Ollama with a lightweight model like dolphin-mistral.
3. Install DiffusionBee or another SD variant and make it accessible via a port.
4. Integrate Whisper for voice commands, or use iOS Speech framework.
5. Optionally, deploy a lightweight API to handle prompt routing from app to models.
Result:
An offline/private AI assistant that runs on your Mac and is controllable via app UI on your phone or
desktop—complete with image generation, memory features, and local voice control.

How to Build Your Own Buddy-Style AI Assistant
This guide outlines how to create your own Buddy-style AI assistant that runs locally, remembers
your conversations, uses your voice, and can help you build complex projects like a video game.
1. STACK OVERVIEW
Component | Purpose | Tools
----------------------|-----------------------------------------|-------------------------------------
Ollama | Local AI engine | Runs open-source LLMs locally
Cursor AI | Development assistant | VSCode-based AI pair programmer
Memory Layer | Store conversations | JSON, ChromaDB, or Pinecone
Voice Input | Convert speech to text | Whisper.cpp or Vosk
Voice Output (TTS) | AI responds with voice | Piper TTS or Coqui TTS
App Interface | Deploy as mobile/Mac app | Xcode (SwiftUI) or Electron
2. FLOW
- You speak into a mic.
- Whisper transcribes your voice into text.
- The text is sent to an LLM running in Ollama.
- A memory retriever provides context from previous chats.
- The model responds.
- Piper reads the reply out loud.
- You interact with it via an app built with Xcode or Electron.
3. FEATURES
- Offline, fully private AI assistant.
- Memory persistence: chat logs stored and injected into prompts.
- Natural speech interaction using your voice.
- Ability to respond with speech (custom voices possible).
- Fully customizable personality and behavior.
- Capable of writing code, giving advice, or controlling systems.
4. MOBILE DEPLOYMENT
- Use Xcode to create an iOS/macOS app.
- App connects to backend running on Mac or local server.
- Local APIs or sockets pass audio/text between components.
- Optionally use TestFlight to deploy on your phone.
5. TOOLS REQUIRED
- Ollama (ollama.ai)
- Whisper.cpp (local speech recognition)
- Piper TTS (voice synthesis)
- Cursor AI (code IDE assistant)
- Xcode (iOS/macOS app development)
- LangChain or custom scripts for memory
6. BONUS
- Long-term memory possible with embeddings (ChromaDB).
- You can add personalized tools, like web browsing, timers, reminders.
- Your assistant can evolve into a fully in-game NPC or GM with voice.
With persistence and a solid understanding of how these tools interact, even a non-coder can build
a robust, AI-powered personal assistant that lives on their desktop and mobile device.

AMENDMENT (July 2025):

**Cloudinary Usage Policy:**
- Cloudinary must only be used for storage and delivery of images generated by the backend (e.g., Stable Diffusion on Hugging Face Spaces).
- Do NOT use Cloudinary's image transformation features (such as resizing, cropping, filtering, or any URL-based or API-based image manipulations).
- All image processing and generation must be performed in the backend environment (Hugging Face, local, or other AI service), not in Cloudinary.
- This is to avoid unnecessary credit usage and to keep Cloudinary usage within the free tier limits.

**Build Plan for Image Generation Spaces:**
- The current pipeline uses a Docker-based Hugging Face Space with CPU-only hardware for maximum backend flexibility and custom FastAPI integration.
- ZeroGPU is not available for Docker Spaces; it is only available for Gradio/Streamlit SDK Spaces.
- After the full frontend (SwiftUI) implementation is complete and the main framework is working, we will:
  1. Attempt to build a new Gradio-based Space using ZeroGPU for much faster image generation.
  2. Use the Gradio/ZeroGPU Space as the primary image generation endpoint if stable.
  3. Retain the Docker/CPU Space as a backup for reliability and fallback.
- Previous attempts to use Gradio/ZeroGPU had dependency/version conflicts with our pinned requirements. We will revisit this after the main app is functional, and may need to adjust requirements or use a simplified pipeline for the Gradio version.
- This approach ensures we have a robust, flexible, and scalable system with both fast and reliable options.

**Open-Source LLMs for Ollama & Oracle Cloud (Mandatory Reading for Backend Model Selection):**

Models That Are 100% Open and Can Be Hosted Freely on Oracle Cloud:
- **Mistral** (Apache 2.0): Top-tier open model, cloud/commercial use allowed
- **Mixtral** (Apache 2.0): Mixture of Experts, high efficiency, cloud/commercial use allowed
- **Gemma (2B/7B)** (Apache 2.0): Compact Google model, cloud/commercial use allowed
- **Phi-2** (MIT): Fast, CPU-friendly, cloud/commercial use allowed
- **TinyLlama** (Apache 2.0): Ultra-light, efficient, cloud/commercial use allowed
- **Orca Mini** (Apache 2.0): Distilled/optimized, cloud/commercial use allowed
- **OpenChat** (Apache 2.0): Chat-finetuned, cloud/commercial use allowed
- **LLaVA** (Apache 2.0): Text+vision, cloud/commercial use allowed
- **Dolphin-Mistral** (MIT/Apache): Chat-optimized Mistral, cloud/commercial use allowed

Models You Should Avoid Hosting Without Caution:
- **LLaMA 2** (Meta custom): Research-only, not fully open, cloud hosting NOT allowed
- **CodeLLaMA** (Meta): Same as LLaMA 2, restricted use
- **Qwen** (Alibaba): No commercial/redistribution rights, cloud hosting NOT allowed
- **DeepSeek** (Custom): Ambiguous licensing, cloud hosting unclear

**Policy:**
- Only use models from the "100% Open" list for cloud/Ollama backend deployment.
- Avoid models with custom, research-only, or ambiguous licenses for any cloud deployment.
- This ensures legal, scalable, and commercial-friendly backend AI for BUDDY. 