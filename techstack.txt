Device: MacBook Pro (13-inch, 2020)
Chip: Apple M1
Memory: 16 GB
Display: 13.3-inch Retina (2560 x 1600)
Storage: 1TB SSD (892.95 GB available)
OS: macOS Sequoia Version 15.4

---

# BUDDY AI Assistant - Current Tech Stack (2025)

## Local Components (Your Mac)
Frontend: SwiftUI (iOS & macOS)
Backend: FastAPI (Python) running locally on localhost:8000
LLMs: Ollama (running locally on localhost:11434, supports Mistral, TinyLlama, Phi-2)
Image Generation: Hugging Face Space (CPU-based, cloud)
Image Storage: Cloudinary (free tier)
Code Formatting: Prettier (JS/TS), SwiftFormat (Swift)
Design: Modern, sleek, visually striking UI (SwiftUI best practices)

## Cloud Components
Hugging Face Space: CPU-based image generation (chiefmaybe-buddy-sd.hf.space)
Cloudinary: Image storage and CDN delivery

## Development Tools
Homebrew: macOS package manager (for installing system dependencies)
Node.js: JavaScript runtime (installed via Homebrew)
npm: Node.js package manager (bundled with Node.js)
Prettier: Code formatter for backend (installed as dev dependency)
Miniconda: conda 25.5.1 (Apple Silicon) - Python environment management

## Backend Configuration
Backend Directory: /backend
Backend Dependencies: FastAPI, requests, cloudinary, uvicorn
Frontend: SwiftUI app scaffolded as 'BUDDYApp' using Swift Package Manager
Workers: 4 (parallel/concurrent requests)
Port: 8000

## Current Architecture
SwiftUI App → Local FastAPI Backend → Local Ollama (chat) / Hugging Face Space (images) → Cloudinary (storage)

## Environment Variables
CLOUDINARY_CLOUD_NAME=your_cloud_name
CLOUDINARY_API_KEY=your_api_key
CLOUDINARY_API_SECRET=your_api_secret

## Current Status
- ✅ Local chat pipeline working (SwiftUI → FastAPI → Ollama)
- ✅ Cloud image generation working (SwiftUI → FastAPI → Hugging Face Space → Cloudinary)
- ✅ SwiftUI frontend connected
- ✅ Cloudinary integration working
- ✅ Backend proxy functionality working

## Performance Specifications
Chat Response Time: < 5 seconds (local Ollama)
Image Generation Time: 5-10 minutes (CPU-based Hugging Face Space)
Backend Startup: < 10 seconds (no ML libraries)
Memory Usage: Minimal (lightweight proxy)

## Future Migration Path
- Containerize backend and Ollama with Docker
- Deploy to paid cloud server (Render, Paperspace, AWS, etc.)
- Move environment variables/secrets to cloud host
- Update frontend to use cloud API endpoint

---

Device: 2020 MacBook Pro M1
OS: macOS Sequoia
Storage: 1TB SSD
Cursor Version: 1.1.6
Xcode Version: 16.4

Project Name: B.U.D.D.Y (Basic Utility Droid Designed for You)
Current Implementation: Local-first with cloud image generation 

## July 2025 UI Update
- Animated buddy avatar (bobbing, green glow when thinking)
- Glowing 'please wait while I am thinking...' bubble
- Custom app icon and avatar (droid theme)
- Modern, card-based UI with chat bubbles and image generator
- All assets provided at 1x, 2x, 3x for crisp display 